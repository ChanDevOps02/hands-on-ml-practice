{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4c4875-5644-41b5-9248-623ab7ccc5cc",
   "metadata": {},
   "source": [
    "## 5. 데이터 정제 및 스케일링 자동화\n",
    "타겟변수와의 correlation이 더 높은 파생변수의 생성, 데이터 정제 및 스케일링을 자동화 및 축약 시켜놓으면 코드의 길이가 짧아지고 가독성이 좋아진다. pipeline 및 columntransformer의 사용에 대해 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec44a6b-69c6-45d8-80bd-1012300bb026",
   "metadata": {},
   "source": [
    "### (1) 파생변수 자동생성 전처리기 구현\n",
    "sklearn의 fit, transform, predict 구현철학을 그대로 가져와 전처리 역할을 자동화 해주는 클래스를 사전에 구현해놓으면 간단한 객체생성만으로도 데이터셋에 파생변수를 빠르게 집어넣을 수 있다. sklearn.base의 BaseEstimator을 상속하면 get_params,set_params 메서드를 사용할 수 있으며 TransformerMixin을 상속하면 fit_transform메서드를 사용할 수 있다. \n",
    "\n",
    " *get_params : 세팅할 수 있는 하이퍼파라미터 목록을 출력하는 메서드\n",
    " \n",
    " *set_params : 하이퍼파라미터를 직접 변경하는 메서드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f704b44-fc95-472f-b5b2-404790f7f6f1",
   "metadata": {},
   "source": [
    "> ℹ️ **부가 설명 | sklearn 구현철학**\n",
    " - fit() : 데이터에 필요한 통계수치등을 학습하여 내부 객체에 저장\n",
    " - transform() : fit()으로 얻은 수치들을 실제 데이터셋에 적용\n",
    " - predict() : 학습된 모델 파라미터를 이용해서 입력값 X에 대한 예측값 y를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91e26fa6-3cbd-4779-8124-5d42059ed46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room :\n",
    "            bedrooms_per_household = X[:, bedrooms_ix] / X[:, households_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_household]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d83e29b3-af3f-4760-82d2-f4a0ee851b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room = False)\n",
    "housing_extra_attr = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5873051c-400d-4db6-88ca-491304a0c01d",
   "metadata": {},
   "source": [
    "### (2) 데이터 스케일링 및 파이프라인을 통한 자동화\n",
    "수치형 데이터들의 범위가 제각각이면 모델 학습시 어려움이 있을 수 있다. 이를 방지하기 위해 sklearn.preprocessing의 min-max-scaling, standardization을 사용할 수 있다. 아래 예시에서는 StandardScaler을 통해 모든 데이터들의 값 범위를 0~1로 제한한다. 또한 sklearn.pipeline의 Pipeline을 통해 Imputer객체를 통한 결측치 대체, 파생변수 생성, 스케일링을 자동화할 수 있다. Pipeline객체의 fit_transform메서드를 실행하면 파라미터 데이터셋에 데이터정제 및 스케일링이 자동적용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "37d5b828-e8b3-4ef2-b069-b29fc9b14177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "    ('attr_adder', CombinedAttributesAdder(add_bedrooms_per_room = False)),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "])\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad04e25-d918-4341-aa3f-a3d4def76062",
   "metadata": {},
   "source": [
    "### (3) 문자 범주형 데이터 전처리 자동화\n",
    "sklearn.compose의 ColumnTransformer객체를 통해 문자 범주형 데이터에 대한 전처리 또한 자동화할 수 있다. 데이터셋의 column명들을 수치형, 문자형으로 나누어 리스트화 시킨 후 ColumnTransformer객체를 통해 수치형 데이터의 전처리 뿐만 아니라 문자형 데이터의 전처리 또한 간단하게 처리할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "61229358-3601-4ecb-b387-5f290b64d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attrib = list(housing_num)\n",
    "cat_attrib = ['ocean_proximity']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_attrib), \n",
    "    ('cat', OneHotEncoder(), cat_attrib)\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7bad8-a23d-467a-b02b-b482434d3c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354b18e-623d-4bce-9e49-18182db8d843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
